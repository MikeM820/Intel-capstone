{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "517edf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 17:30:55.706449: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tensorflow import keras\n",
    "from keras.layers import LSTM, Dense, Dropout, TimeDistributed\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a0e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_helper(row):\n",
    "    delta = datetime.timedelta(hours=1)\n",
    "    if row['Time_diff_sec'] < row['sec_to_next_hr']:\n",
    "        row['End'] = row['Start'] + pd.to_timedelta(row['Time_diff_sec'], unit='S')\n",
    "        return [row]\n",
    "    row2 = row.copy()\n",
    "    \n",
    "    row['End'] = (row['Start']+delta).floor('H')\n",
    "    row2['Start'] = row['End']\n",
    "    row2['End'] = (row['End']+delta).floor('H')\n",
    "    \n",
    "    row2['Time_diff_sec'] = row['Time_diff_sec'] - row['sec_to_next_hr']\n",
    "    row2['sec_to_next_hr'] = 3600\n",
    "    row['Time_diff_sec'] = row['sec_to_next_hr']\n",
    "    return [row] + row_helper(row2)\n",
    "\n",
    "def clean_row(row):\n",
    "    if row['Time_diff_sec'] > row['sec_to_next_hr']:\n",
    "        return pd.DataFrame(row_helper(row))\n",
    "    return pd.DataFrame([row])\n",
    "\n",
    "def clean_dataset(file_path):\n",
    "    df = pd.read_csv(file_path, parse_dates=['Start', 'End'])\n",
    "    df['Duration'] = df['Duration'].apply(lambda x: pd.Timedelta(x))\n",
    "    df['Time_diff_sec'] = df['Duration'].apply(lambda x: x.total_seconds())\n",
    "    df = df.drop(columns='Duration')\n",
    "    delta = datetime.timedelta(hours=1)\n",
    "    df['sec_to_next_hr'] = df['Start'].apply(lambda x: ((x+delta).replace(microsecond=0, second=0, minute=0) - x).seconds)\n",
    "\n",
    "    return pd.concat([clean_row(row) for _, row in df.iterrows()], ignore_index=True)\n",
    "\n",
    "def get_dataset(df, lookback):\n",
    "    df['weekday'] = df['Start'].apply(lambda x: x.dayofweek)#.astype('category')\n",
    "    df['hour'] = df['Start'].apply(lambda x: x.hour)#.astype('category')\n",
    "    df['minute'] = df['Start'].apply(lambda x: x.minute)\n",
    "    df['date'] = df['Start'].apply(lambda x: x.day)\n",
    "    df['month'] = df['Start'].apply(lambda x: x.month)\n",
    "    df = df.drop(columns='Start')\n",
    "#     df = pd.get_dummies(df).values\n",
    "    df = df.values\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(df)-lookback-1):\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = df[i:i+lookback, 1:], df[i+lookback-1, 0:1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(y)\n",
    "    y = scaler.transform(y)\n",
    "    return np.array(X), np.array(y), scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee536f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_2:\n",
    "    def __init__(self, args) -> None:\n",
    "        self.args = args\n",
    "#         experiment = args['experiment']\n",
    "#         self.dir_path = f'../../../outputs/LSTM_{experiment}'\n",
    "#         if not os.path.exists(self.dir_path):\n",
    "#             os.mkdir(self.dir_path)\n",
    "#         with open(f'{self.dir_path}/config.json', 'w') as file:\n",
    "#             json.dump(args, file)\n",
    "    \n",
    "    def train(self):\n",
    "        file_path = \"/Users/yikaimao/Desktop/DSC_180B/Intel-capstone/data/processed/lstm_dataset_local.csv\"\n",
    "        print('Processing dataset...')\n",
    "        df = clean_dataset(file_path)\n",
    "        self.input_df = df[df['Value'] == self.args['exe_name']].reset_index()\n",
    "        self.input_df = self.input_df.groupby(pd.Grouper(key='Start', freq='H')).sum().reset_index()\n",
    "        X, y, self.scaler = get_dataset(self.input_df, self.args['lookback'])\n",
    "\n",
    "        self.train_size = int(X.shape[0] * 0.8)\n",
    "        self.X_train, self.X_test = X[:self.train_size, :, :], X[self.train_size:, :, :]\n",
    "        self.y_train, self.y_test = y[:self.train_size], y[self.train_size:]\n",
    "\n",
    "        feature_shape = self.X_train.shape[2]\n",
    "\n",
    "        self.model = keras.Sequential()\n",
    "        self.model.add(LSTM(32, return_sequences=True, input_shape=(self.args['lookback'], feature_shape)))\n",
    "        # self.model.add(Dropout(0.2))\n",
    "\n",
    "        self.model.add(LSTM(32, return_sequences=True))\n",
    "        # self.model.add(Dropout(0.2))\n",
    "\n",
    "        self.model.add(LSTM(16, return_sequences=True))\n",
    "        # self.model.add(Dropout(0.2))\n",
    "\n",
    "        self.model.add(LSTM(16))\n",
    "        # self.model.add(Dropout(0.2))\n",
    "\n",
    "        # self.model.add(TimeDistributed(Dense(1)))\n",
    "        self.model.add(Dense(32))\n",
    "        self.model.add(Dense(16))\n",
    "        self.model.add(Dense(1))\n",
    "        opt = keras.optimizers.Adam(learning_rate=self.args['learning_rate'])\n",
    "        self.model.compile(optimizer=opt, loss=self.args['loss'])\n",
    "\n",
    "        print('Training model...')\n",
    "        self.history = self.model.fit(self.X_train, self.y_train, epochs=self.args['epochs'], verbose=2)\n",
    "        print('Finished training.')\n",
    "\n",
    "        train_loss = self.model.evaluate(self.X_train, self.y_train)\n",
    "        print(f'Total loss: {train_loss}')\n",
    "\n",
    "        print('Plotting the loss over epoch')\n",
    "        loss = self.history.history['loss']\n",
    "        loss_layout = go.Layout(\n",
    "            title='Loss plot',\n",
    "            xaxis={'title':'Epoch'},\n",
    "            yaxis={'title':'Loss'}\n",
    "        )\n",
    "        loss_fig = go.Figure([\n",
    "            go.Scatter(x=list(range(self.args['epochs'])), y=loss, mode='lines',name = 'training loss')\n",
    "        ], layout=loss_layout)\n",
    "        loss_fig.show()\n",
    "\n",
    "#         print('Saving everything...')\n",
    "#         with open(f'{self.dir_path}/train_history.json', 'w') as file:\n",
    "#             json.dump(self.history.history, file)\n",
    "#         self.model.save(f'{self.dir_path}/model.h5')\n",
    "#         pio.write_image(loss_fig, f'{self.dir_path}/loss.png', width=985, height=525)\n",
    "#         print(f'Model, model history, loss plot saved at {self.dir_path}')\n",
    "\n",
    "    def evaluate(self):\n",
    "        test_loss = self.model.evaluate(self.X_test, self.y_test)\n",
    "        print(f'Test loss: {test_loss}')\n",
    "\n",
    "        print('Plotting the prediction and ground truth')\n",
    "        train_pred = self.scaler.inverse_transform(self.model.predict(self.X_train, verbose=0))\n",
    "        test_pred = self.scaler.inverse_transform(self.model.predict(self.X_test, verbose=0))\n",
    "\n",
    "        pred_layout = go.Layout(\n",
    "            title='Firefox used in seconds per hour',\n",
    "        #     xaxis={'title':'Date'},\n",
    "            yaxis={'title':'Duration(s)'}\n",
    "        )\n",
    "        pred_fig = go.Figure([\n",
    "            go.Scatter(x=self.input_df['Start'], y=self.input_df['Time_diff_sec'].values, name='ground truth'),\n",
    "            go.Scatter(x=self.input_df['Start'].iloc[:self.train_size], y=train_pred[:,0], name='train prediction'),\n",
    "            go.Scatter(x=self.input_df['Start'].iloc[self.train_size:], y=test_pred[:,0], name='test prediction')\n",
    "        ], layout=pred_layout)\n",
    "        pred_fig.show()\n",
    "\n",
    "#         pio.write_image(pred_fig, f'{self.dir_path}/prediction.png', width=985, height=525)\n",
    "#         print(f'Prediction plot saved at {self.dir_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e38c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
